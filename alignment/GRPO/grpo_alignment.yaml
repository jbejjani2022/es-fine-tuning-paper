model_name: /n/netscratch/kempner_sham_lab/Lab/itamarf/es-fine-tuning-paper/models/alpaca-7b
train_jsonl: /n/home07/itamarf/es-fine-tuning-paper/alignment/data/custom_train_250.jsonl
eval_jsonl: /n/home07/itamarf/es-fine-tuning-paper/alignment/data/custom_eval_500.jsonl
prompt_key: prompt
scorer_url: http://holygpu8a15102:8000

# ---------- Lambda / constraint (now matched to ES defaults) ----------
lambda_cost: 0.5              # initial λ
lambda_adapt: true            # turn on Lagrangian adaptation
cost_threshold_d: 0.0         # J_C = E[C] + d  (same as ES run_accl script)
lambda_lr: 0.005              # learning rate for ln λ, same as ES
lambda_min: 1.0e-4
lambda_max: 5.0
jc_ema_beta: 0.9
lambda_pos_cost_only: true    # only update on positive violations

# ---------- Scorer / logging convenience ----------
scorer_batch_size: 16         # batch size used during training reward calls
scorer_batch_size_eval: 32    # batch size for scorer during eval pass
eval_batch_size: 8            # prompts per eval generate batch
scatter_every: 50             # steps between train scatter logs
hist_every: 25                # steps between train histogram logs
scatter_max_points: 2000      # max points in each scatter

# ---------- GRPO / generation ----------
num_generations: 30
max_prompt_length: 512
max_completion_length: 512
temperature: 0.7
top_p: 1.0

learning_rate: 5.0e-6
lr_scheduler_type: constant
warmup_steps: 0
num_train_epochs: 1
max_steps: 500
gradient_accumulation_steps: 15
per_device_train_batch_size: 1
per_device_eval_batch_size: 2

project: grpo_alignment
entity: itamarf
output_dir: /n/netscratch/kempner_sham_lab/Lab/itamarf/es-fine-tuning-paper/alignment/GRPO_output
save_steps: 100
logging_steps: 5

loss_type: grpo
