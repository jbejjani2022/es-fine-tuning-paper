# TRL GRPO sweep config for reproducing the Conciseness table (GRPO only)
# Model: Qwen/Qwen2.5-7B-Instruct
# We sweep over beta (KL strength) and seed.

model_name: Qwen/Qwen2.5-7B-Instruct

# Dataset: a JSONL file with either {"prompt": "..."} or {"question": "..."}.
# Your earlier path:
train_jsonl: /n/home07/itamarf/es-fine-tuning-paper/conciseness/data/train.jsonl
# Optional held-out for reporting reward/KL (if omitted, trainer logs are used):
eval_jsonl: /n/home07/itamarf/es-fine-tuning-paper/conciseness/data/eval.jsonl
prompt_key: question
solution_key: answer

# Group size (generations per prompt)
num_generations: 30

# Tokenization / lengths
max_prompt_length: 128
max_completion_length: 64

# Decoding
temperature: 0.7
top_p: 1.0

prompt_key: question
solution_key: answer

# Optim
learning_rate: 5.0e-6
num_train_epochs: 1
max_steps: 1000            # leave -1 if using epochs
gradient_accumulation_steps: 30

# Logging / output
project: es_conciseness
entity: itamarf
output_dir: /n/netscratch/kempner_sham_lab/Lab/itamarf/es-fine-tuning-paper/GRPO
save_steps: 200             # we'll save on epoch end
logging_steps: 5

# DeepSpeed (optional): path to a ds_config.json if you want ZeRO-3. Leave empty to skip.
deepspeed_config: ""

# Sweep
betas: [0.0, 0.01, 0.0167, 0.0464]
seeds: [11, 22, 33, 44]

# Reward config for conciseness
reward:
  # target length in tokens used to scale reward; shorter than this is better but we guard against empty/garbage
  target_len: 20
  min_alnum_frac: 0.2    # if fewer than this fraction of chars are alphanumeric -> reward=0 (anti-hack)
  empty_penalty: 0.0     # set to 0 (hard zero) for empty/whitespace completions
  clip_min: 0.0
  clip_max: 1.0

# Trainer niceties
scale_rewards: batch     # 'group' | 'batch' | 'none'
loss_type: dr_grpo
mask_truncated_completions: false